{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 보스턴 주택 가격 예측\n",
    "- 1970년대 중반 보스턴 외곽 지역의 범죄율, 지방세율 등의 데이터가 주어졌을 때 주택 가격의 중간 값을 예측한다.\n",
    "- 데이터 포인트가 506개로 개수가 적고 404개는 훈련 샘플, 102개는 테스트 샘플로 나뉘어 있다.\n",
    "- 입력 데이터에 있는 각 피처는 스케일이 서로 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보스턴 주택 데이터셋 로드하기\n",
    "1. 1 인당 범죄율.\n",
    "2. 25,000 평방 피트 이상의 부지에 대해 구역화 된 주거용 토지의 비율.\n",
    "3. 도시 당 비 소매 비즈니스 에이커의 비율.\n",
    "4. Charles River 더미 변수 (지역이 강 경계면 = 1, 그렇지 않으면 0).\n",
    "5. 산화 질소 농도 (1,000 만분 율).\n",
    "6. 주거 당 평균 방 수.\n",
    "7. 1940 년 이전에 지어진 소유주 소유 유닛의 비율.\n",
    "8. 5 개의 보스턴 고용 센터까지의 가중 거리.\n",
    "9. 방사형 고속도로에 대한 접근성 색인.\n",
    "10. $ 10,000 당 전체 가치 재산 세율.\n",
    "11. 도시 별 학생-교사 비율.\n",
    "12. 1000 * (Bk-0.63) ** 2 여기서 Bk는 도시 별 흑인 비율입니다.\n",
    "13. 인구의 낮은 지위 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "(train_data,train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 404개의 훈련 샘플과 102개의 테스트 샘플이 있고 13개의 수치형 피처가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
       "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
       "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
       "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
       "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
       "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
       "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
       "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
       "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
       "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
       "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
       "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
       "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
       "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
       "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
       "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
       "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
       "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
       "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
       "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
       "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
       "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
       "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
       "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
       "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
       "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
       "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
       "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
       "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
       "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
       "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
       "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
       "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
       "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
       "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
       "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
       "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 타깃은 주택의 중간 가격으로 천 달러 단위이다.\n",
    "- 가격은 일반적으로 1만 달러에서 5만 달러 사이이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정규화하기\n",
    "- 스케일이 다른 피처를 신경망에 주입하면 전역 최소 점을 찾아가는 경사 하강법의 경로가 스케일이 큰 특성에 영향을 많이 받기 때문에 학습을 어렵게 한다.\n",
    "- 입력 데이터를 표준화(standardization)한다 즉, 입력 데이터에 있는 각 피처에 대해 피처의 평균을 빼고 표준 편차로 나눈다.\n",
    "- 피처의 중앙이 0 근처에 맞춰지고 표준 편차는 1이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#평균을 뺀 값을 표준 편차로 나눈다\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 테스트 데이터를 정규화할 때 사용한 값이 훈련 데이터에서 계산한 값이다.\n",
    "- 머신 러닝 작업 과정에서 절대로 테스트 데이터에서 계산한 어떤 값도 사용해서는 안 된다.\n",
    "- 마찬가지로 실전에 투입하여 새로운 데이터에 대한 예측을 만들 때도 훈련 데이터에서 계산한 값을 사용하여 정규화해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의하기\n",
    "- 샘플 개수가 적기 때문에 64개의 유닛을 가진 2개의 은닉 층으로 작은 네트워크를 구성하여 사용한다.\n",
    "- 일반적으로 훈련 데이터의 개수가 적을수록 과대적합이 더 쉽게 일어나므로 작은 모델을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model(): #동일한 모델을 여러 번 생성할 것이므로 함수를 만들어 사용합니다.\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "404/404 [==============================] - 0s 274us/step - loss: nan - mae: nan   \n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 0s 99us/step - loss: nan - mae: nan\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 0s 99us/step - loss: nan - mae: nan\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 5/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: nan - mae: nan\n",
      "Epoch 6/50\n",
      "404/404 [==============================] - 0s 106us/step - loss: nan - mae: nan\n",
      "Epoch 7/50\n",
      "404/404 [==============================] - 0s 111us/step - loss: nan - mae: nan\n",
      "Epoch 8/50\n",
      "404/404 [==============================] - 0s 141us/step - loss: nan - mae: nan\n",
      "Epoch 9/50\n",
      "404/404 [==============================] - 0s 101us/step - loss: nan - mae: nan\n",
      "Epoch 10/50\n",
      "404/404 [==============================] - 0s 106us/step - loss: nan - mae: nan\n",
      "Epoch 11/50\n",
      "404/404 [==============================] - 0s 96us/step - loss: nan - mae: nan\n",
      "Epoch 12/50\n",
      "404/404 [==============================] - 0s 106us/step - loss: nan - mae: nan\n",
      "Epoch 13/50\n",
      "404/404 [==============================] - 0s 96us/step - loss: nan - mae: nan\n",
      "Epoch 14/50\n",
      "404/404 [==============================] - 0s 104us/step - loss: nan - mae: nan\n",
      "Epoch 15/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 16/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: nan - mae: nan\n",
      "Epoch 17/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 18/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 19/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 20/50\n",
      "404/404 [==============================] - 0s 94us/step - loss: nan - mae: nan\n",
      "Epoch 21/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 22/50\n",
      "404/404 [==============================] - 0s 86us/step - loss: nan - mae: nan\n",
      "Epoch 23/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: nan - mae: nan\n",
      "Epoch 24/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 25/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: nan - mae: nan\n",
      "Epoch 26/50\n",
      "404/404 [==============================] - 0s 94us/step - loss: nan - mae: nan\n",
      "Epoch 27/50\n",
      "404/404 [==============================] - 0s 94us/step - loss: nan - mae: nan\n",
      "Epoch 28/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 29/50\n",
      "404/404 [==============================] - 0s 101us/step - loss: nan - mae: nan\n",
      "Epoch 30/50\n",
      "404/404 [==============================] - 0s 101us/step - loss: nan - mae: nan\n",
      "Epoch 31/50\n",
      "404/404 [==============================] - 0s 94us/step - loss: nan - mae: nan\n",
      "Epoch 32/50\n",
      "404/404 [==============================] - 0s 87us/step - loss: nan - mae: nan\n",
      "Epoch 33/50\n",
      "404/404 [==============================] - 0s 93us/step - loss: nan - mae: nan\n",
      "Epoch 34/50\n",
      "404/404 [==============================] - 0s 96us/step - loss: nan - mae: nan\n",
      "Epoch 35/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 36/50\n",
      "404/404 [==============================] - 0s 99us/step - loss: nan - mae: nan\n",
      "Epoch 37/50\n",
      "404/404 [==============================] - 0s 106us/step - loss: nan - mae: nan\n",
      "Epoch 38/50\n",
      "404/404 [==============================] - 0s 104us/step - loss: nan - mae: nan\n",
      "Epoch 39/50\n",
      "404/404 [==============================] - 0s 111us/step - loss: nan - mae: nan\n",
      "Epoch 40/50\n",
      "404/404 [==============================] - 0s 104us/step - loss: nan - mae: nan\n",
      "Epoch 41/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 42/50\n",
      "404/404 [==============================] - 0s 93us/step - loss: nan - mae: nan\n",
      "Epoch 43/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 44/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: nan - mae: nan\n",
      "Epoch 45/50\n",
      "404/404 [==============================] - 0s 101us/step - loss: nan - mae: nan\n",
      "Epoch 46/50\n",
      "404/404 [==============================] - 0s 88us/step - loss: nan - mae: nan\n",
      "Epoch 47/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: nan - mae: nan\n",
      "Epoch 48/50\n",
      "404/404 [==============================] - 0s 96us/step - loss: nan - mae: nan\n",
      "Epoch 49/50\n",
      "404/404 [==============================] - 0s 84us/step - loss: nan - mae: nan\n",
      "Epoch 50/50\n",
      "404/404 [==============================] - 0s 88us/step - loss: nan - mae: nan\n",
      "102/102 [==============================] - 0s 283us/step\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_targets, epochs=50, batch_size=16)\n",
    "val_mse, val_mae = model.evaluate(test_data, test_targets)\n",
    "print(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 네트워크의 마지막 층은 하나의 유닛을 가지고 있고 활성화 함수가 없다.(선형 층)\n",
    "- 활성화 함수를 적용하면 출력 값의 범위를 제한하게 되기 때문이다.\n",
    "- 마지막 층이 순수한 선형이므로 네트워크가 어떤 범위의 값이라도 예측하도록 자유롭게 학습된다.\n",
    "- 전형적인 스칼라 회귀(하나의 연속적인 값을 예측하는 회귀)를 위한 구성이다.\n",
    "- 평균 제곱 오차(mse)를 손실 함수로 사용한다.\n",
    "- 평균 절대 오차(mae)를 모니터링을 위한 성능 평가 지표로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-겹 검증하기\n",
    "- 하이퍼 파라미터를 튜닝하면서 모델을 평가하기 위해 데이터를 훈련 세트와 검증 세트로 나눈다.\n",
    "- 데이터 포인트가 많지 않기 때문에 검증 세트가 매우 작아지고 어떤 데이터 포인트가 훈련 세트와 검증 세트로 선택되었는지에 따라 점수가 크게 달라진다.\n",
    "- 검증 세트의 분할에 대한 검증 점수의 분산이 높으면 신뢰 있는 모델 평가를 할 수 없다.\n",
    "- 따라서 K-겹 교차 검증을 사용한다.\n",
    "- 모델의 검증 점수는 K개의 검증 점수 평균이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리중인 폴드 # 0\n",
      "처리중인 폴드 # 1\n",
      "처리중인 폴드 # 2\n",
      "처리중인 폴드 # 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('처리중인 폴드 #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]  # 검증 데이터 준비: k번째 분할\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate(  # 훈련 데이터 준비: 다른 분할 전체\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "model = build_model()  # 케라스 모델 구성(컴파일 포함)\n",
    "model.fit(partial_train_data, partial_train_targets,  # 모델 훈련(verbose=0이므로 훈련 과정이 출력되지 않습니다.)\n",
    "          epochs=num_epochs, batch_size=1, verbose=0)\n",
    "val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)  # 검증 세트로 모델 평가\n",
    "all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_epochs=100으로 실행했다.\n",
    "- 검증 점수가 2.0에서 3.0까지 변화가 크다.\n",
    "- 평균값(2.6)이 각각의 점수보다 훨씬 신뢰할 만하다.\n",
    "- 평균적으로 2,600달러 정도 차이가 난다.\n",
    "- 주택 가격의 범위가 10,000달러에서 50,000달러 사이인 것을 감안하면 비교적 큰 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 폴드에서 검증 점수를 로그에 저장하기\n",
    "- 신경망을 조금 더 오래 500 에포크 동안 훈련한다.\n",
    "- 각 에포크마다 모델이 얼마나 개선되는지 기록하기 위해 훈련 루프를 수정하여 에포크의 검증 점수를 로그에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리중인 폴드 # 0\n",
      "처리중인 폴드 # 1\n",
      "처리중인 폴드 # 2\n",
      "처리중인 폴드 # 3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_mean_absolute_error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-46b28437d038>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     epochs=num_epochs, batch_size=1, verbose=0)\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmae_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_mean_absolute_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mall_mae_histories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmae_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_mean_absolute_error'"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print('처리중인 폴드 #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]  #검증 데이터 준비: k번째 분할    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(  # 훈련 데이터 준비: 다른 분할 전체\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "model = build_model()  # 케라스 모델 구성(컴파일 포함)\n",
    "history = model.fit(partial_train_data, partial_train_targets,  # 모델 훈련(verbose=0이므로 훈련 과정이 출력되지 않습니다.)\n",
    "                    validation_data=(val_data, val_targets),\n",
    "                    epochs=num_epochs, batch_size=1, verbose=0)\n",
    "mae_history = history.history['val_mean_absolute_error']\n",
    "all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-겹 검증 점수 평균 기록하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\testAI\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\testAI\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 75\n"
     ]
    }
   ],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증 점수 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 처음 10개의 데이터 포인트를 제외한 검증 점수 그리기\n",
    "- 곡선의 다른 부분과 스케일이 많이 다른 첫 10개의 데이터 포인트를 제외시킨다.\n",
    "- 부드러운 곡선을 얻기 위해 각 포인트를 이전 포인트의 지수 이동 평균으로 대체한다.\n",
    "- 지수 이동 평균은 시계열 데이터를 부드럽게 만드는 기법 중 하나로 이전에 계산된 이동 평균에 factor를 곱하고 현재 포인트에 (1-factor)를 곱해 합산한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검증 MAE가 60번째 에포크 이후에 줄어드는 것이 멈췄다.\n",
    "- 이 지점 이후로 과대적합이 시작된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()  # 새롭게 컴파일된 모델을 얻습니다.\n",
    "model.fit(train_data, train_targets,  # 전체 데이터로 훈련시킵니다.\n",
    "          epochs=80, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
